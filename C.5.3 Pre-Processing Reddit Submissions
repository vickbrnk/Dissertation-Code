library(dplyr)
library(tidyverse)
library(mnormt)
library(psych)
library(textclean)
#--------------------------------------------------------
#Pre-Processing Time!
#--------------------------------------------------------
setwd("/Users/vicky/Desktop/Dissertation/Data/1Final/ForPre-Pr")
dfRedSub <- read_csv("FinalRedditSubmissions.csv")

#copy column for processed text
dfRedSub$FinalText <- dfRedSub$Combine

#1.	Lowercase
dfRedSub$FinalText  <- tolower(dfRedSub$FinalText)

#2.	Remove Twitter Handles (@)
dfRedSub$FinalText  <- gsub("@\\w+", "", dfRedSub$FinalText)

#3.	Remove hashtags
dfRedSub$FinalText  <- gsub("#\\w+", "", dfRedSub$FinalText)

#4. Remove html sequences (&gt)
dfRedSub$FinalText  <- gsub("&\\w+", "", dfRedSub$FinalText )

#5.	Remove URLs
dfRedSub$FinalText <- gsub("https?://.+", "", dfRedSub$FinalText)

#6.	Replace contractions
#117 contractions rather than the 70 default
cont2<- read_csv("contractions.csv")
#fix the ' difference
cont2$contraction <-gsub("'", "â€™", cont2$contraction)

dfRedSub$FinalText  <- replace_contraction(dfRedSub$FinalText)
dfRedSub$FinalText  <- replace_contraction(dfRedSub$FinalText,contraction.key = cont2)

#7.	Remove emojis or encode?
#remove
dfRedSub$NOemojiTxt <- gsub("[^\x01-\x7F]", "", dfRedSub$FinalText)

#encode emojis
dfRedSub$EmojiText <- replace_emoji(dfRedSub$FinalText,)
#some encoded, others not, in format of <XX>, remove these
dfRedSub$EmojiText  <- gsub("<.*?>", "", dfRedSub$EmojiText)

#8.Remove punctuation
dfRedSub$NOemojiTxt <- gsub("[[:punct:]]", "", dfRedSub$NOemojiTxt)
dfRedSub$EmojiText  <- gsub("[[:punct:]]", "", dfRedSub$EmojiText)

#9	Remove numbers & special characters
dfRedSub$NOemojiTxt <- gsub("[[:digit:]]", "", dfRedSub$NOemojiTxt)
dfRedSub$EmojiText  <- gsub("[[:digit:]]", "", dfRedSub$EmojiText)

#10. Remove anything but standard ASCII just in case
dfRedSub$NOemojiTxt <- gsub("[^\x01-\x7F]", "", dfRedSub$NOemojiTxt)
dfRedSub$EmojiText  <- gsub("[^\x01-\x7F]", "", dfRedSub$EmojiText)

#11.	Remove spaces and new lines
dfRedSub$NOemojiTxt <- gsub("\n", " ", dfRedSub$NOemojiTxt)
dfRedSub$NOemojiTxt <- gsub("^\\s+", "", dfRedSub$NOemojiTxt)
dfRedSub$NOemojiTxt <- gsub("\\s+$", "", dfRedSub$NOemojiTxt)
dfRedSub$NOemojiTxt <- gsub("[ |\t]+", " ", dfRedSub$NOemojiTxt)

dfRedSub$EmojiText <- gsub("\n", " ", dfRedSub$EmojiText)
dfRedSub$EmojiText <- gsub("^\\s+", "", dfRedSub$EmojiText)
dfRedSub$EmojiText <- gsub("\\s+$", "", dfRedSub$EmojiText)
dfRedSub$EmojiText <- gsub("[ |\t]+", " ", dfRedSub$EmojiText)


#12.	All English language
library("cld3")
dfRedSub$language <- detect_language(dfRedSub$NOemojiTxt)
dfRedSub$languageEmoji <- detect_language(dfRedSub$EmojiText)

#use non emoji observations, check if make a difference
all.equal(dfRedSub$language,dfRedSub$languageEmoji)
#"'is.NA' value mismatch: 33148 in current 33173 in target"
#less missmatches in emoji at 33148
sapply(dfRedSub , function(x) sum(is.na(x)))
#less missing values with emoji language at 33173, languageEmoji at 33148

unique(dfRedSub$language)
unique(dfRedSub$languageEmoji)
otherlang <- dfRedSub[dfRedSub$language != "en",]
nrow(otherlang)
sapply(otherlang , function(x) sum(is.na(x)))
#44586 some mixed with other language and some appear to be just too short to be classified
otherlang2 <- dfRedSub[dfRedSub$languageEmoji != "en",]
nrow(otherlang2)
sapply(otherlang2 , function(x) sum(is.na(x)))
#44536 some mixed with other language and some appear to be just too short to be classified

#not just english
dfRedSub2 <- dfRedSub[dfRedSub$language == "en",]
nrow(dfRedSub2)
#501546
dfRedSub3 <- dfRedSub[dfRedSub$languageEmoji == "en",]
nrow(dfRedSub3)
#501571
#pick emoji since less NA values
Englishdf <- select(dfRedSub3, -c(language,languageEmoji))
sapply(Englishdf , function(x) sum(is.na(x)))
Englishdf2 <- Englishdf[complete.cases(Englishdf),]
nrow(Englishdf2)
#468423/512959, 0.9131782, about 9% reduction


#13.	Remove stop words
library(tm)
Englishdf2$EmojiText = removeWords(Englishdf2$EmojiText, stopwords("english"))
Englishdf2$NOemojiTxt = removeWords(Englishdf2$NOemojiTxt, stopwords("english"))

#14.	Remove spaces and new lines
Englishdf2$NOemojiTxt <- gsub("\n", " ", Englishdf2$NOemojiTxt)
Englishdf2$NOemojiTxt <- gsub("^\\s+", "", Englishdf2$NOemojiTxt)
Englishdf2$NOemojiTxt <- gsub("\\s+$", "", Englishdf2$NOemojiTxt)
Englishdf2$NOemojiTxt <- gsub("[ |\t]+", " ", Englishdf2$NOemojiTxt)

Englishdf2$EmojiText <- gsub("\n", " ", Englishdf2$EmojiText)
Englishdf2$EmojiText <- gsub("^\\s+", "", Englishdf2$EmojiText)
Englishdf2$EmojiText <- gsub("\\s+$", "", Englishdf2$EmojiText)
Englishdf2$EmojiText <- gsub("[ |\t]+", " ", Englishdf2$EmojiText)

#15.	Lemmatization
library(textstem)
Englishdf2$LemEmojitxt <- lemmatize_strings(Englishdf2$EmojiText)
Englishdf2$LemNOEmojitxt <- lemmatize_strings(Englishdf2$NOemojiTxt)


#some are numbers, cus base of second is 2
num1 <- Englishdf2[grepl('[[:digit:]]', Englishdf2$LemEmojitxt),]
nrow(num1)
#33032, 33032/512959 = 0.06439501, affects about 6.4% of the dataset
num2 <- Englishdf2[grepl('[[:digit:]]', Englishdf2$LemNOEmojitxt),]
nrow(num1)
#33032, 33032/512959 = 0.06439501, affects about 6.4% of the dataset

#numbers?, remove these and note this in the steps
Englishdf2$LemEmojitxt   <- gsub("[[:digit:]]", "", Englishdf2$LemEmojitxt)
Englishdf2$LemNOEmojitxt  <- gsub("[[:digit:]]", "", Englishdf2$LemNOEmojitxt )

#16.	Stemming
library("SnowballC")
Englishdf2$StemEmojitxt <- stemDocument(Englishdf2$EmojiText)
Englishdf2$StemNOEmojitxt <- stemDocument(Englishdf2$NOemojiTxt)

#17.	Remove spaces and new lines
#NOemojiTxt
Englishdf2$NOemojiTxt <- gsub("\n", " ", Englishdf2$NOemojiTxt)
Englishdf2$NOemojiTxt <- gsub("^\\s+", "", Englishdf2$NOemojiTxt)
Englishdf2$NOemojiTxt <- gsub("\\s+$", "", Englishdf2$NOemojiTxt)
Englishdf2$NOemojiTxt <- gsub("[ |\t]+", " ", Englishdf2$NOemojiTxt)

#EmojiText
Englishdf2$EmojiText <- gsub("\n", " ", Englishdf2$EmojiText)
Englishdf2$EmojiText <- gsub("^\\s+", "", Englishdf2$EmojiText)
Englishdf2$EmojiText <- gsub("\\s+$", "", Englishdf2$EmojiText)
Englishdf2$EmojiText <- gsub("[ |\t]+", " ", Englishdf2$EmojiText)

#LemEmojitxt
Englishdf2$LemEmojitxt <- gsub("\n", " ", Englishdf2$LemEmojitxt)
Englishdf2$LemEmojitxt <- gsub("^\\s+", "", Englishdf2$LemEmojitxt)
Englishdf2$LemEmojitxt <- gsub("\\s+$", "", Englishdf2$LemEmojitxt)
Englishdf2$LemEmojitxt <- gsub("[ |\t]+", " ", Englishdf2$LemEmojitxt)

#LemNOEmojitxt
Englishdf2$LemNOEmojitxt <- gsub("\n", " ", Englishdf2$LemNOEmojitxt)
Englishdf2$LemNOEmojitxt <- gsub("^\\s+", "", Englishdf2$LemNOEmojitxt)
Englishdf2$LemNOEmojitxt <- gsub("\\s+$", "", Englishdf2$LemNOEmojitxt)
Englishdf2$LemNOEmojitxt <- gsub("[ |\t]+", " ", Englishdf2$LemNOEmojitxt)

#StemNOEmojitxt
Englishdf2$StemNOEmojitxt <- gsub("\n", " ", Englishdf2$StemNOEmojitxt)
Englishdf2$StemNOEmojitxt <- gsub("^\\s+", "", Englishdf2$StemNOEmojitxt)
Englishdf2$StemNOEmojitxt <- gsub("\\s+$", "", Englishdf2$StemNOEmojitxt)
Englishdf2$StemNOEmojitxt <- gsub("[ |\t]+", " ", Englishdf2$StemNOEmojitxt)

#StemEmojitxt
Englishdf2$StemEmojitxt <- gsub("\n", " ", Englishdf2$StemEmojitxt)
Englishdf2$StemEmojitxt <- gsub("^\\s+", "", Englishdf2$StemEmojitxt)
Englishdf2$StemEmojitxt <- gsub("\\s+$", "", Englishdf2$StemEmojitxt)
Englishdf2$StemEmojitxt <- gsub("[ |\t]+", " ", Englishdf2$StemEmojitxt)

#save so far
write_csv(Englishdf2, file = "PauseRedditSubmissions.csv")

#18.	Remove posts with less than 4 words
#check length of text
Englishdf2$StemEmojitxt <- as.character(Englishdf2$StemEmojitxt)
Englishdf2$LENStemEmojitxt<- str_count(Englishdf2$StemEmojitxt,'\\w+')
#check length of text
Englishdf2$StemNOEmojitxt <- as.character(Englishdf2$StemNOEmojitxt)
Englishdf2$LENStemNOEmojitxt <- str_count(Englishdf2$StemNOEmojitxt,'\\w+')
#check length of text
Englishdf2$LemEmojitxt <- as.character(Englishdf2$LemEmojitxt)
Englishdf2$LENLemEmojitxt<- str_count(Englishdf2$LemEmojitxt,'\\w+')
#check length of text
Englishdf2$LemNOEmojitxt <- as.character(Englishdf2$LemNOEmojitxt)
Englishdf2$LENLemNOEmojitxt <- str_count(Englishdf2$LemNOEmojitxt,'\\w+')
#check length of text
Englishdf2$EmojiText <- as.character(Englishdf2$EmojiText)
Englishdf2$LENEmojitxt<- str_count(Englishdf2$EmojiText,'\\w+')
#check length of text
Englishdf2$NOemojiTxt <- as.character(Englishdf2$NOemojiTxt)
Englishdf2$LENLNOEmoji <- str_count(Englishdf2$NOemojiTxt,'\\w+')

summary(Englishdf2)
#average is 64 words per post for all, with a median of 20 Lowest appears to be in LENLemNOEmojitxt

test<- Englishdf2[Englishdf2$LENStemNOEmojitxt<3,]
nrow(test)
#285/512959 = 0.0005556,  almost 0.05 %
summary(test)
#most messages incoherent
test2<- Englishdf2[Englishdf2$LENStemNOEmojitxt<4,]
nrow(test2)
#1470/512959 = 0.002865726,  almost 0.3 %
#incoherent messages

#-----------
Final <- Englishdf2[Englishdf2$LENStemNOEmojitxt>4,]
nrow(Final)
#463438
# reduction of 463438/512959=0.9034601, 10%
#save so far
setwd("/Users/vicky/Desktop/Dissertation/Data/1Final/After prep")
write_csv(Final, file = "FinalRedditSubmissions.csv")

#---------------------
#generate word cloud
#---------------------
library(tidytext)
library(wordcloud)
#NOemojiTxt, about 88 words at 20k
cloudNOemojiTxt <- Final %>% 
  mutate(linenumber=row_number()) %>% 
  unnest_tokens(word, NOemojiTxt) %>% anti_join(stop_words)

cloudNOemojiTxt1 <- cloudNOemojiTxt %>% count(word, sort=TRUE)

cloudNOemojiTxt1 %>% with(wordcloud(word, n, 
                                    min.freq=20000, random.order=FALSE))

#EmojiText
cloudEmojiText <- Final %>% 
  mutate(linenumber=row_number()) %>% 
  unnest_tokens(word, EmojiText) %>% anti_join(stop_words)

cloudEmojiText1 <- cloudEmojiText %>% count(word, sort=TRUE)

cloudEmojiText1 %>% with(wordcloud(word, n, 
                                   min.freq=20000, random.order=FALSE))

#LemEmojitxt
cloudLemEmojitxt <- Final %>% 
  mutate(linenumber=row_number()) %>% 
  unnest_tokens(word, LemEmojitxt) %>% anti_join(stop_words)

cloudLemEmojitxt1 <- cloudLemEmojitxt %>% count(word, sort=TRUE)

cloudLemEmojitxt1 %>% with(wordcloud(word, n, 
                                     min.freq=20000, random.order=FALSE))

#LemNOEmojitxt- NOT DONE
cloudLemNOEmojitxt <- Final %>% 
  mutate(linenumber=row_number()) %>% 
  unnest_tokens(word, LemNOEmojitxt) %>% anti_join(stop_words)

cloudLemNOEmojitxt1 <- cloudLemNOEmojitxt %>% count(word, sort=TRUE)

cloudLemNOEmojitxt1 %>% with(wordcloud(word, n, 
                                       min.freq=20000, random.order=FALSE))

#StemNOEmojitxt
cloudStemNOEmojitxt <- Final %>% 
  mutate(linenumber=row_number()) %>% 
  unnest_tokens(word, StemNOEmojitxt) %>% anti_join(stop_words)

cloudStemNOEmojitxt1 <- cloudStemNOEmojitxt %>% count(word, sort=TRUE)

cloudStemNOEmojitxt1 %>% with(wordcloud(word, n, 
                                        min.freq=20000, random.order=FALSE))

#StemEmojitxt
cloudStemEmojitxt <- Final %>% 
  mutate(linenumber=row_number()) %>% 
  unnest_tokens(word, StemEmojitxt) %>% anti_join(stop_words)

cloudStemEmojitxt1 <- cloudStemEmojitxt %>% count(word, sort=TRUE)

cloudStemEmojitxt1 %>% with(wordcloud(word, n, 
                                      min.freq=20000, random.order=FALSE))



